{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make five implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Your dataset should be sufficiently large for a neural network to perform well (samples should really be in the thousands here) and try to pick something that takes advantage of neural networks’ ability to have both feature extraction and supervised capabilities, so don’t pick something with an easy to consume list of features already generated for you (though neural networks can still be useful in those contexts).\n",
    "\n",
    "Note that if you want to use an unprocessed image dataset, scikit-image is a useful package for converting to importable numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project will use the CIFAR10 small image classification data set from Keras. It is a dataset of 50,000 32x32 color training images labeled over 10 categories, and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3) (50000, 1) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Images are 32 x 32 x 3 (RGB), so reshape to 50,000, 3072 and 10,000, 3072\n",
    "x_train = x_train.reshape(50000, 3072)\n",
    "x_test = x_test.reshape(10000, 3072)\n",
    "# Splitting labels into ten categories\n",
    "# Normalizing the x features\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train  /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1: Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 201,482\n",
      "Trainable params: 201,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# Output shape should be equal to the number of classes (10)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 2s - loss: 2.0338 - acc: 0.2473     \n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.9197 - acc: 0.2948     \n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.8852 - acc: 0.3110     \n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.8664 - acc: 0.3203     \n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.8526 - acc: 0.3264     \n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.8422 - acc: 0.3305     \n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.8250 - acc: 0.3396     \n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.8078 - acc: 0.3456     \n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.7981 - acc: 0.3473     \n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.7909 - acc: 0.3531     \n",
      "Test Loss:  1.72420504169\n",
      "Test Accuracy:  0.3626\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Implementation 1:\n",
    "   Our model did not perform well. It took around twenty seconds and only had an accuracy of 36.26%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 2: Multi Layer Perceptron\n",
    "\n",
    "Changing optimizer to Adadelta based on results from http://cs231n.github.io/neural-networks-3/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 201,482\n",
      "Trainable params: 201,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Modifying the original MLP\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# Output shape should be equal to the number of classes (10)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.9952 - acc: 0.2706     \n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.8309 - acc: 0.3372     \n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.7690 - acc: 0.3643     \n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.7309 - acc: 0.3811     \n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.7013 - acc: 0.3908     \n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.6734 - acc: 0.4033     \n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.6521 - acc: 0.4087     \n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 2s - loss: 1.6340 - acc: 0.4167     \n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.6140 - acc: 0.4262     \n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.5985 - acc: 0.4323     \n",
      "Test Loss:  1.62469896145\n",
      "Test Accuracy:  0.4186\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Implementation 2:\n",
    "Using the adadelta optimizer improved the accuracy to 41.86%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 3: Multi Layer Perceptron\n",
    "Using Adadelta optimizer and leaky relu activation. Leaky ReLU is explained here  https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 201,482\n",
      "Trainable params: 201,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maken\\Anaconda3\\lib\\site-packages\\keras\\activations.py:103: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  ).format(identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# Modifying the original MLP\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation=keras.layers.LeakyReLU(), input_shape=(3072,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation=keras.layers.LeakyReLU()))\n",
    "model.add(Dropout(0.1))\n",
    "# Output shape should be equal to the number of classes (10)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.9634 - acc: 0.2890     \n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.8186 - acc: 0.3501     \n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.7571 - acc: 0.3746     \n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.7144 - acc: 0.3935     \n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.6808 - acc: 0.4066     \n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.6567 - acc: 0.4130     \n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.6325 - acc: 0.4206     \n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.6181 - acc: 0.4292     \n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.5992 - acc: 0.4362     \n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 3s - loss: 1.5815 - acc: 0.4397     \n",
      "Test Loss:  1.54988387165\n",
      "Test Accuracy:  0.4508\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Implementation 3:\n",
    "Using adadelta optimizer and leaky ReLU activations increased the accuray to 45.08%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 4: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 131s - loss: 1.7523 - acc: 0.3699   \n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 128s - loss: 1.3678 - acc: 0.5133   \n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 128s - loss: 1.2131 - acc: 0.5737   \n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 130s - loss: 1.1163 - acc: 0.6070   \n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 132s - loss: 1.0486 - acc: 0.6316   \n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 139s - loss: 0.9893 - acc: 0.6557   \n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 133s - loss: 0.9459 - acc: 0.6717   \n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 133s - loss: 0.9040 - acc: 0.6842   \n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 128s - loss: 0.8696 - acc: 0.6979   \n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 128s - loss: 0.8331 - acc: 0.7105   \n",
      "Test loss: 0.899023178005\n",
      "Test accuracy: 0.687\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 32, 32\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Convolutional Neural Network\n",
    "Implementing a CNN improved accuracy to 68.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 5: Hierarchical Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 295s - loss: 2.0196 - acc: 0.2585 - val_loss: 1.8669 - val_acc: 0.3305\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 294s - loss: 1.8232 - acc: 0.3359 - val_loss: 1.7692 - val_acc: 0.3554\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 296s - loss: 1.7392 - acc: 0.3674 - val_loss: 1.7275 - val_acc: 0.3676\n",
      "Test loss: 1.72748284378\n",
      "Test accuracy: 0.3676\n"
     ]
    }
   ],
   "source": [
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# The data, shuffled and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 32, 32, 3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32, 32, 3)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Implementation 5:\n",
    "The Hierarchical Recurrent Neural Network implementation took about five minutes per epoch and resulted in an accuracy of 36.76%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results:\n",
    "\n",
    "Original Multi-Layer Perceptron: 2 seconds/epoch, 36.26% accuracy.\n",
    "\n",
    "MLP Implementation 2: 2-3 seconds/epoch, 31.86% accuracy.\n",
    "\n",
    "MLP Implementation 3: 2-3 seconds/epoch, 45.08% accuracy.\n",
    "\n",
    "CNN Implementation: 130 seconds/epoch, 68.7% accuracy.\n",
    "\n",
    "RNN Implementation: 295 seconds/epoch, 36.76% accuracy.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "The CNN Implementation returned the best accuracy with a large increase in computational time. The results were compared against a baseline of randomly choosing the category (10%). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
